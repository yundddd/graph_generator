{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependency if in colab environment. Not required for local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab\")\n",
    "    !pip install torch==2.4.0\n",
    "    !pip install torch_geometric\n",
    "else:\n",
    "    print(\"Not running in Google Colab\")\n",
    "    # For mac, this has to be set once to enable multiprocessing dataloader.\n",
    "    import multiprocessing\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"fork\")\n",
    "    except RuntimeError:\n",
    "        print(f\"Context has alrady been set: {multiprocessing.get_start_method()}\")\n",
    "    assert multiprocessing.get_start_method() == \"fork\"\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from functools import lru_cache\n",
    "from math import floor\n",
    "import mmap\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# seed everything\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['WANDB_API_KEY']='d8e70c9cb01a88ace48a2ec6d18bd9e9be24c73b'\n",
    "os.environ['WANDB_ENTITY']='yundddd-stanford-university'\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']='1'\n",
    "wandb.login();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvUODUXl51x4"
   },
   "source": [
    "## Download simulated autonomous vehicle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIMENSION = 9\n",
    "TIMESTAMP_FEATURE_INDEX = 3\n",
    "HEALTHY_NODE_LABEL = 0\n",
    "ROOT_CAUSE_NODE_LABEL = 1\n",
    "\n",
    "class GraphFaultInjectionDataset(Dataset):\n",
    "  \"\"\"\n",
    "\n",
    "  Args:\n",
    "      root: Root directory where the dataset should be saved.\n",
    "      device: Device to use. Must be \"cuda\" or \"mps\".\n",
    "      batch_size: Batch size to use.\n",
    "      force_reload: If set to ``True``, the dataset will be re-downloaded and reprocessed.\n",
    "  \"\"\"\n",
    "  def __init__(self, root: str,  device: str, batch_size: int,force_reload: bool = False):\n",
    "    assert device != \"cpu\", \"CPU ain't gonna cut it\"\n",
    "    self.train_file_idx = []\n",
    "    self.test_file_idx = []\n",
    "    self.raw_data_paths = []\n",
    "    self.file_idx_to_graph_idx_range = []\n",
    "    self.total_num_graphs = 0\n",
    "    self.device = device\n",
    "    self.batch_size = batch_size\n",
    "    super().__init__(root=root, force_reload=force_reload)\n",
    "\n",
    "  #####################################\n",
    "  # PyG Dataset interface\n",
    "  #####################################\n",
    "  # Download entire dataset folder and unzip it into raw_dir.\n",
    "  def download(self):\n",
    "    SOURCE_URL = \"https://github.com/yundddd/graph_generator/raw/master/graph_generator/dataset/autonomous_vehicle.zip\"\n",
    "    zip_path = download_url(url=SOURCE_URL, folder=self.raw_dir)\n",
    "    self._unzip_file(zip_path, self.raw_dir)\n",
    "\n",
    "  # If the raw zip exist, skip download unless force_reload is passed to the constructor.\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      return [\"autonomous_vehicle.zip\"]\n",
    "  \n",
    "  # Always process.\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      return [\"always_process.pt\"]\n",
    "\n",
    "  def process(self):\n",
    "    for subfolder in sorted(os.listdir(os.path.join(self.raw_dir, \"autonomous_vehicle/\"))):\n",
    "      print(\"processing subfolder\", subfolder)\n",
    "      subfolder_path = os.path.join(self.raw_dir, \"autonomous_vehicle/\", subfolder)\n",
    "      edge_idx_path = os.path.join(subfolder_path, 'edge_index.csv')\n",
    "      \n",
    "      subfolder_raw_files = []\n",
    "\n",
    "      for file_name in sorted(os.listdir(subfolder_path)):\n",
    "        if 'inject' in file_name:\n",
    "          injection_time = self._get_injection_time(file_name)\n",
    "          node_features_path = os.path.join(subfolder_path, f'node_feature_inject_at_{injection_time}.csv')\n",
    "          fault_label_path = os.path.join(subfolder_path, f'fault_label_inject_at_{injection_time}.csv')\n",
    "          subfolder_raw_files.append((node_features_path, fault_label_path, edge_idx_path))\n",
    "      \n",
    "      print(f\"collected {len(subfolder_raw_files)} fault injection sweeps from subfolder {subfolder}\")\n",
    "\n",
    "      for node_feature_file, _, _ in subfolder_raw_files:\n",
    "        num_graph = self._count_graphs(node_feature_file)\n",
    "        self.file_idx_to_graph_idx_range.append((self.total_num_graphs, self.total_num_graphs + num_graph - 1))\n",
    "        self.total_num_graphs += num_graph\n",
    "\n",
    "      self.raw_data_paths += subfolder_raw_files\n",
    "      ## construct train/test masks.\n",
    "      train_file_idx, test_file_idx = self._sample_train_test_file_masks(len(subfolder_raw_files), split=0.8)\n",
    "      train_length = len(train_file_idx)\n",
    "      test_length = len(test_file_idx)\n",
    "      self.train_file_idx += [idx + train_length for idx in train_file_idx]\n",
    "      self.test_file_idx += [idx + test_length for idx in test_file_idx]\n",
    "\n",
    "    if self.force_reload or self._should_write_tensor_to_disk():\n",
    "      # saving node_feature tensors\n",
    "      with tqdm(total=len(self.raw_data_paths), desc=f\"Saving feature tensors to disk\", unit=\"files\") as pbar:\n",
    "        for idx in range(len(self.raw_data_paths)):\n",
    "          node_feature, fault_label = self._node_feature_and_label_to_tensor(self.raw_data_paths[idx][0], self.raw_data_paths[idx][1])\n",
    "          torch.save(node_feature, os.path.join(self.processed_dir, f'node_feature_{idx}.pt'))\n",
    "          torch.save(fault_label, os.path.join(self.processed_dir, f'fault_label_{idx}.pt'))\n",
    "          pbar.update(1)\n",
    "      \n",
    "    print(f\"total number of graphs: {self.total_num_graphs}\")\n",
    "    print(f\"total number of raw files: {len(self.raw_data_paths)}\")\n",
    "    assert self.file_idx_to_graph_idx_range[-1][-1] == self.total_num_graphs - 1\n",
    "\n",
    "  def len(self):\n",
    "    return self.total_num_graphs\n",
    "  \n",
    "  def get(self, idx):\n",
    "    graph_offset_in_file = idx - self.file_idx_to_graph_idx_range[self._graph_idx_to_file_idx(idx)][0]\n",
    "    file_idx = self._graph_idx_to_file_idx(idx)\n",
    "\n",
    "    node_feature, fault_label = self._load_feature_and_label(file_idx)\n",
    "    data = Data(x=node_feature[graph_offset_in_file], y=fault_label[graph_offset_in_file], edge_index=self.edge_index)\n",
    "    return data\n",
    "\n",
    "\n",
    "  #####################################\n",
    "  # Helpers\n",
    "  #####################################\n",
    "  \n",
    "  # Get a dataloader for training.\n",
    "  def train_loader(self):\n",
    "    loader = DataLoader(Subset(self, self.train_idx), batch_size=self.batch_size, shuffle=False, num_workers=2, prefetch_factor=2)\n",
    "    return preload_to_gpu(loader, self.device)\n",
    "  \n",
    "  # Get a dataloader for testing.\n",
    "  def test_loader(self):\n",
    "    loader = DataLoader(Subset(self, self.test_idx), batch_size=self.batch_size, shuffle=False, num_workers=2, prefetch_factor=2)\n",
    "    return preload_to_gpu(loader, self.device)\n",
    "  \n",
    "  # Helper to get train file mask\n",
    "  @property  \n",
    "  def train_idx(self):\n",
    "    result = []\n",
    "    for file_idx in self.train_file_idx:\n",
    "        result.extend(self._file_idx_to_graph_idx(file_idx))\n",
    "    return result\n",
    "  \n",
    "  # Helper to get test file mask\n",
    "  @property  \n",
    "  def test_idx(self):\n",
    "    result = []\n",
    "    for file_idx in self.test_file_idx:\n",
    "        result.extend(self._file_idx_to_graph_idx(file_idx))\n",
    "    return result\n",
    "  \n",
    "  # only read edge index from file once since graph structure doesn't change.\n",
    "  @property\n",
    "  @lru_cache(maxsize=32)\n",
    "  def edge_index(self):\n",
    "    return self._get_edge_index(self.raw_data_paths[0][2])\n",
    "  \n",
    "  # If the procssed tensor is not on disk, write all tensor to disk.\n",
    "  def _should_write_tensor_to_disk(self):\n",
    "    return not os.path.exists(self.processed_dir + \"/node_feature_0.pt\")\n",
    "\n",
    "  # Load feature and label tensors from disk. Cache the result to reduce file IO.\n",
    "  @lru_cache(maxsize=32)\n",
    "  def _load_feature_and_label(self, idx):\n",
    "    return (torch.load(os.path.join(self.processed_dir, f'node_feature_{idx}.pt'), weights_only=True, mmap=True),\n",
    "           torch.load(os.path.join(self.processed_dir, f'fault_label_{idx}.pt'), weights_only=True, mmap=True))\n",
    "  \n",
    "  # Each file contains multiple graphs. We need to map a graph index to a file index and vice versa.\n",
    "  def _file_idx_to_graph_idx(self, file_idx):\n",
    "    r = self.file_idx_to_graph_idx_range[file_idx]\n",
    "    return range(r[0], r[1] + 1)\n",
    "    \n",
    "  def _graph_idx_to_file_idx(self, graph_idx):\n",
    "    def find_range_index(ranges, target_range):\n",
    "      return next((i for i, r in enumerate(ranges) if r[0] <= target_range and r[1] >= target_range), -1)\n",
    "    return find_range_index(self.file_idx_to_graph_idx_range, graph_idx)\n",
    "\n",
    "  # Sample train/test file masks evenly for each fault.\n",
    "  def _sample_train_test_file_masks(self, data_length: int, split: float = 0.8) -> Tuple[List[int], List[int]]:\n",
    "    train_idx = sorted(random.sample(range(data_length), floor(data_length * split)))\n",
    "    val_idx = sorted(list(set(range(data_length)) - set(train_idx)))\n",
    "    return train_idx, val_idx\n",
    "\n",
    "  # Count the number of graphs from a feature file.\n",
    "  def _count_graphs(self, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Memory-map the file (0 means the entire file)\n",
    "        mmapped_file = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
    "        \n",
    "        # Count occurrences of newline characters (\\n)\n",
    "        line_count = 0\n",
    "        while True:\n",
    "            byte = mmapped_file.readline()\n",
    "            if not byte:\n",
    "                break\n",
    "            line_count += 1\n",
    "        return line_count\n",
    "\n",
    "  def _get_edge_index(self, path):\n",
    "      edges = []\n",
    "      with open(path, \"r\") as output:\n",
    "        i = 0\n",
    "        for line in output:\n",
    "          edge = line.strip().split(',')\n",
    "          edges.append([int(edge[0]), int(edge[1])])\n",
    "      return torch.tensor(edges, dtype=torch.long).t().contiguous() # put edges into COO format\n",
    "\n",
    "  def _get_node_at_fault(self, path):\n",
    "    node_at_fault, timestamp_of_fault = 0, 0\n",
    "    with open(path, \"r\") as output:\n",
    "      ind = 0\n",
    "      for line in output:\n",
    "        node_at_fault, timestamp_of_fault = line.strip().split(',')\n",
    "        ind += 1\n",
    "      assert ind == 1 # there should only be one line in this file\n",
    "    return int(node_at_fault), int(timestamp_of_fault)\n",
    "  \n",
    "  def _get_injection_time(self, file_name: str):\n",
    "    return file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "  # Read raw feature and label files from disk and then compute the tensors.\n",
    "  @lru_cache(maxsize=32)\n",
    "  def _node_feature_and_label_to_tensor(self, node_features_path, fault_label_path) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    node_features, node_labels = [], []\n",
    "    node_at_fault, timestamp_of_fault = self._get_node_at_fault(fault_label_path)\n",
    "    with open(node_features_path, \"r\") as output:\n",
    "      with mmap.mmap(output.fileno(), length=0, access=mmap.ACCESS_READ) as mm:\n",
    "        for line in iter(mm.readline, b\"\"):  # Read lines from mmap:\n",
    "          line = line.decode('utf-8').strip()\n",
    "          items = line.strip().split(',')\n",
    "          features = [list(map(int, items[i + 1 : i + FEATURE_DIMENSION + 1]))\n",
    "                            for i in range(0, len(items), FEATURE_DIMENSION + 1)]\n",
    "          time = max([node[TIMESTAMP_FEATURE_INDEX] for node in features])\n",
    "          node_features.append(features)\n",
    "          labels = [HEALTHY_NODE_LABEL] * len(features)\n",
    "          if time >= timestamp_of_fault:\n",
    "            labels[node_at_fault] = ROOT_CAUSE_NODE_LABEL\n",
    "          node_labels.append(labels)\n",
    "\n",
    "    node_labels = torch.tensor(node_labels, dtype=torch.long)\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    # TODO: use sparse tensor\n",
    "    #indices = torch.nonzero(node_labels, as_tuple=False).t()\n",
    "    #values = node_labels[indices[0]]\n",
    "    #sparse_label = torch.sparse_coo_tensor(indices, values, node_labels.size())\n",
    "    return node_features, node_labels\n",
    "            \n",
    "  def _unzip_file(self,zip_path: str, extract_to :str) -> None:\n",
    "    print(\"unzipping\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Unzipped to '{extract_to}'\")\n",
    "    \n",
    "# A helper to preload the data to GPU. This can significantly speed up training.\n",
    "def preload_to_gpu(loader, device):\n",
    "    out = []\n",
    "    with tqdm(total=len(loader), desc=f\"Preloading data to GPU\", unit=\"batch\") as pbar:\n",
    "        for batch in loader:\n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "            except Exception as e:\n",
    "                assert False, f\"GPU memory is full. Please restart kernel. Error: {e}\"\n",
    "            out.append(batch)\n",
    "            pbar.update(1)\n",
    "    return DataLoader(out, shuffle=False)\n",
    "  \n",
    "def print_gpu_usage(device):\n",
    "    if device == \"cuda\":\n",
    "        print(f\"GPU memory usage: {torch.cuda.memory_allocated(device) / 1024**3} GB / {torch.cuda.get_device_properties(device).total_memory / 1024**3} GB\")\n",
    "    elif device == \"mps\":\n",
    "         # Total memory allocated by tensors on MPS\n",
    "        allocated_memory = torch.mps.current_allocated_memory()\n",
    "        print(f\"Allocated memory: {allocated_memory / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Total memory reserved by PyTorch for MPS\n",
    "        reserved_memory = torch.mps.driver_allocated_memory()\n",
    "        print(f\"Reserved memory: {reserved_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    assert False, \"No GPU detected\"\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "print_gpu_usage(device)\n",
    "\n",
    "my_dataset = GraphFaultInjectionDataset(root=\"/tmp\", batch_size=4096, device=device, force_reload=False)\n",
    "\n",
    "train_loader = my_dataset.train_loader()\n",
    "test_loader = my_dataset.test_loader()\n",
    "print_gpu_usage(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    SAGEConv,\n",
    "    GCNConv,\n",
    "    GATConv,\n",
    "    GATv2Conv\n",
    ")\n",
    "\n",
    "def build_preprocessing_layers(input_features:int, hidden_dim:int, num_layer: int):\n",
    "    preprocessing_layers = [torch.nn.Linear(input_features, hidden_dim)]\n",
    "    for _ in range(1, num_layer):\n",
    "        preprocessing_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        preprocessing_layers.append(torch.nn.ReLU())\n",
    "    return torch.nn.Sequential(*preprocessing_layers)\n",
    "\n",
    "def build_postprocessing_layers(output_classes:int, hidden_dim:int, num_layer: int):\n",
    "    post_processing_layers = []\n",
    "    for _ in range(num_layer - 1):\n",
    "        post_processing_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        post_processing_layers.append(torch.nn.ReLU())\n",
    "    post_processing_layers.append(torch.nn.Linear(hidden_dim, output_classes))\n",
    "    return torch.nn.Sequential(*post_processing_layers)\n",
    "\n",
    "class DirectedGCN(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim, output_classes, num_preprocessing, num_postprocessing, **kwarg):\n",
    "        super().__init__()\n",
    "        self.preprocessing = build_preprocessing_layers(input_features, hidden_dim, num_preprocessing)\n",
    "        self.post_processing = build_postprocessing_layers(output_classes, hidden_dim, num_postprocessing)\n",
    "        self.norm = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim, improved=True)  #improved option for directed graphs\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim, improved=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.preprocessing(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.post_processing(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class DirectedGraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim, output_classes, num_preprocessing, num_postprocessing, **kwarg):\n",
    "        super().__init__()\n",
    "        self.preprocessing = build_preprocessing_layers(input_features, hidden_dim, num_preprocessing)\n",
    "        self.post_processing = build_postprocessing_layers(output_classes, hidden_dim, num_postprocessing)\n",
    "        self.norm = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv1 = SAGEConv(hidden_dim, hidden_dim, normalize=True)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim, normalize=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.preprocessing(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.dropout(x)\n",
    "        x = self.post_processing(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class DirectedGAT(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim, output_classes, num_preprocessing, num_postprocessing, heads=4, **kwarg):\n",
    "        super().__init__()\n",
    "        self.preprocessing = build_preprocessing_layers(input_features, hidden_dim, num_preprocessing)\n",
    "        self.post_processing = build_postprocessing_layers(output_classes, hidden_dim, num_postprocessing)\n",
    "        self.norm = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv1 = GATv2Conv(hidden_dim, hidden_dim, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=1, concat=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.preprocessing(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.post_processing(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class LSTMGNN(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim, output_classes, num_preprocessing, num_postprocessing, num_lstm_layers=1, **kwarg\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        # preprocessing and post processing\n",
    "        self.preprocessing = build_preprocessing_layers(input_features, hidden_dim, num_preprocessing)\n",
    "        self.post_processing = build_postprocessing_layers(output_classes, hidden_dim, num_postprocessing)\n",
    "\n",
    "        self.norm = torch.nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # First GCN layer\n",
    "        self.conv1 = GATv2Conv(hidden_dim, hidden_dim, heads=1, concat=False)\n",
    "\n",
    "        # LSTM for node-level temporal processing\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Second GCN layer that uses updated node embeddings\n",
    "        self.conv2 = GATv2Conv(hidden_dim, hidden_dim, heads=1, concat=False)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch  # Graph inputs\n",
    "\n",
    "        # Preprocess node features\n",
    "        x = self.preprocessing(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Reshape for LSTM: [batch_size, node_size, hidden_dim]\n",
    "        batch_size = batch.max().item() + 1\n",
    "        node_size = x.size(0) // batch_size\n",
    "        x = x.view(batch_size, node_size, -1)\n",
    "\n",
    "        # Reset hidden state if necessary\n",
    "    \n",
    "        self.hidden_state = (\n",
    "            torch.zeros(self.num_lstm_layers, batch_size, self.hidden_dim, device=x.device),\n",
    "            torch.zeros(self.num_lstm_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        )\n",
    "\n",
    "        # LSTM processing\n",
    "        lstm_out, self.hidden_state = self.lstm(x, self.hidden_state)  # [batch_size, node_size, hidden_dim]\n",
    "\n",
    "        # Flatten for second GCN: [total_nodes, hidden_dim]\n",
    "        x = lstm_out.reshape(-1, self.hidden_dim)\n",
    "\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Add dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer for node classification\n",
    "        x = self.post_processing(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "MODELS = {\n",
    "    'DirectedGCN': DirectedGCN,\n",
    "    'DirectedGraphSAGE': DirectedGraphSAGE,\n",
    "    'DirectedGAT': DirectedGAT,\n",
    "    'LSTMGNN': LSTMGNN,\n",
    "    #'GRNN': GRNN\n",
    "}\n",
    "\n",
    "def build_model(model_name, **kwargs):\n",
    "    model = MODELS[model_name](**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping utility to stop training when the test loss does not improve.\n",
    "\n",
    "    Args:\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "        delta (float): Minimum change in test loss to be considered an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, test_loss):\n",
    "        \"\"\"\n",
    "        Call method to check if training should stop.\n",
    "\n",
    "        Args:\n",
    "            test_loss (float): Current test loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if early stopping is triggered, False otherwise.\n",
    "        \"\"\"\n",
    "        if test_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = test_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "def train(model, train_loader, test_loader, num_epoch, lr=0.00001, weight_decay=5e-4, loss_function=F.nll_loss):\n",
    "    plt.figure()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    early_stopping = EarlyStopping(patience=3, delta=1e-4)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    print(f\"Training {num_epoch} epochs with {len(train_loader)} batches each\")\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f\"Train Epoch {epoch+1}/{num_epoch}\", unit=\"batch\") as pbar:\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                pbar.update(1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch)\n",
    "                loss = loss_function(out, batch.y)\n",
    "\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                optimizer.step()\n",
    "                \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with tqdm(total=len(test_loader), desc=f\"Testing\", unit=\"batch\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    pbar.update(1)\n",
    "                    out = model(batch)\n",
    "                    loss = loss_function(out, batch.y)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        \n",
    "        wandb.log({\"train_loss\": train_losses[-1], \"test_loss\": test_losses[-1], \"epoch\": epoch})\n",
    "        \n",
    "        if len(test_losses) > 10 and early_stopping(test_losses[-1]):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Clear the plot before updating\n",
    "        plt.clf()  # Clear the figure to avoid accumulating legends and line\n",
    "        clear_output(wait=True)\n",
    "        # Update the plot dynamically\n",
    "        epochs = list(range(1, len(train_losses) + 1))\n",
    "        plt.yscale('log')\n",
    "        plt.gca().yaxis.set_major_locator(LogLocator(base=10.0, numticks=10))\n",
    "\n",
    "        plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "        plt.plot(epochs, test_losses, label='Test Loss', color='red')\n",
    "        last_train_loss = train_losses[-1]\n",
    "        last_test_loss = test_losses[-1]\n",
    "        plt.text(epochs[-1], last_train_loss, f'{last_train_loss:.4f}', color='blue', ha='left', va='top')\n",
    "        plt.text(epochs[-1], last_test_loss, f'{last_test_loss:.4f}', color='red', ha='right', va='top')\n",
    "\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Test Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)  # Optional: add a grid\n",
    "        plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def evaluate(model, model_name, loader):\n",
    "    classification_threshold, host_targets, host_raw_probs = find_best_threshold(model, loader)\n",
    "    host_preds = (host_raw_probs > classification_threshold).long()\n",
    "    \n",
    "    class0 = 1 - host_raw_probs\n",
    "    class1 = host_raw_probs\n",
    "    y_true = (host_targets == 1).numpy()\n",
    "    wandb.log({\"roc\": wandb.plot.roc_curve(y_true, torch.stack((class0, class1), dim=1))})\n",
    "    report = classification_report(host_targets, host_preds, digits=4, zero_division=0, output_dict=True)\n",
    "    assert isinstance(report, dict)\n",
    "    print(f'Model: {model_name}')\n",
    "    print(report)\n",
    "    wandb.log({\n",
    "        \"0 - precision\": report[\"0\"][\"precision\"],\n",
    "        \"0 - recall\": report[\"0\"][\"recall\"],\n",
    "        \"0 - f1-score\": report[\"0\"][\"f1-score\"],\n",
    "        \"1 - precision\": report[\"1\"][\"precision\"],\n",
    "        \"1 - recall\": report[\"1\"][\"recall\"],\n",
    "        \"1 - f1-score\": report[\"1\"][\"f1-score\"]})\n",
    "\n",
    "    cm = wandb.plot.confusion_matrix(y_true=y_true, preds=host_preds.numpy(), class_names=['False', 'True'])\n",
    "    wandb.log({\"conf_mat\": cm})\n",
    "\n",
    "def find_best_threshold(model, loader):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    raw_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(loader), desc=f\"Finding the best classification threshold\", unit=\"batch\") as pbar:\n",
    "            for idx,batch in enumerate(loader):\n",
    "                out = model(batch)\n",
    "                raw_prob = torch.exp(out[:,1]) # Get class 1 predictions and convert to probability.\n",
    "                raw_probs.append(raw_prob)\n",
    "                \n",
    "                targets.append(batch.y)\n",
    "                pbar.update(1)\n",
    "\n",
    "    targets = torch.cat(targets)\n",
    "    raw_probs = torch.cat(raw_probs)\n",
    "    \n",
    "    host_targets = targets.to('cpu')\n",
    "    host_raw_probs = raw_probs.to('cpu')\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(host_targets, host_raw_probs)\n",
    "    distances = np.sqrt(fpr**2 + (1 - tpr)**2)\n",
    "    best_idx = np.argmin(distances)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"True Positive Rate: {tpr[best_idx]:.4f}, False Positive Rate: {fpr[best_idx]:.4f}\")   \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label='ROC Curve')\n",
    "    plt.scatter(fpr[best_idx], tpr[best_idx], color='red', label=f'Best Threshold: {best_threshold:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_threshold, host_targets, host_raw_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"model_name\": \"LSTMGNN\",\n",
    "    \"hidden_dim\": FEATURE_DIMENSION * 2,\n",
    "    \"output_classes\": 2,\n",
    "    \"input_features\": FEATURE_DIMENSION,\n",
    "    \"num_preprocessing\": 2,\n",
    "    \"num_postprocessing\": 2,\n",
    "    \"num_lstm_layers\": 1,\n",
    "    \"num_epoch\": 100\n",
    "}\n",
    "model = build_model(training_config[\"model_name\"],\n",
    "                    input_features=training_config[\"input_features\"],\n",
    "                    hidden_dim=training_config[\"hidden_dim\"],\n",
    "                    output_classes=training_config[\"output_classes\"],\n",
    "                    num_preprocessing=training_config[\"num_preprocessing\"],\n",
    "                    num_postprocessing=training_config[\"num_postprocessing\"],\n",
    "                    num_lstm_layers=training_config[\"num_lstm_layers\"])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"cs224w-final-project\",\n",
    "    config=training_config,\n",
    "    name=datetime.now().strftime(\"%m-%d %H:%M:%S \") + training_config[\"model_name\"]\n",
    ")\n",
    "wandb.run.log_code(include_fn=lambda path: path.endswith(\".ipynb\"))  \n",
    "train(model, train_loader, test_loader, num_epoch=training_config[\"num_epoch\"], lr=training_config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, model_name, test_loader)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqMDZ1qte1KQO14M3ABqGr",
   "collapsed_sections": [
    "PheHlalcRe3v",
    "eJ3NyKrk6hal",
    "rvUODUXl51x4",
    "v7iKUu_g7MBD",
    "HR7mKapo7TAa",
    "iLQP2QnbTHIT"
   ],
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
